{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import weakref \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from root_pandas import read_root\n",
    "\n",
    "\n",
    "from data_cleaning import clean_df\n",
    "from KFPF_lambda_cuts import KFPF_lambda_cuts\n",
    "from plot_tools import AMS, preds_prob, plot_confusion_matrix\n",
    "\n",
    "import uproot\n",
    "import gc\n",
    "\n",
    "#To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") \n",
    "        \n",
    "#to paralellize some part of the code\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "signal= pd.DataFrame(data=uproot.open('/home/shahid/cbmsoft/Data/PFSimplePlainTreeSignal.root:PlainTree',\n",
    "                                           library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='np',\n",
    "                                decompression_executor=executor, interpretation_executor=executor))\n",
    "\n",
    "# We only select lambda candidates\n",
    "sgnal = signal[(signal['LambdaCandidates_is_signal']==1) & (signal['LambdaCandidates_mass']>1.108)\n",
    "               & (signal['LambdaCandidates_mass']<1.1227)]\n",
    "\n",
    "# Similarly for the background\n",
    "background = pd.DataFrame(data=uproot.open('/home/shahid/cbmsoft/Data/PFSimplePlainTreeBackground.root:PlainTree',\n",
    "                                           library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='np',\n",
    "                                decompression_executor=executor, interpretation_executor=executor))\n",
    "bg = background[(background['LambdaCandidates_is_signal'] == 0)\n",
    "                & ((background['LambdaCandidates_mass'] > 1.07)\n",
    "                & (background['LambdaCandidates_mass'] < 1.108) | (background['LambdaCandidates_mass']>1.1227) \n",
    "                   & (background['LambdaCandidates_mass'] < 2.00))]\n",
    "\n",
    "#delete unused variables\n",
    "del signal\n",
    "del background\n",
    "\n",
    "#we also import a 10k events data set, generated using URQMD with AuAu collisions at 12AGeV\n",
    "file = uproot.open('/home/shahid/cbmsoft/Data/10k_events_PFSimplePlainTree.root:PlainTree', library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='np',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "#Call the python garbage collector to clean up things\n",
    "gc.collect()\n",
    "df_original= pd.DataFrame(data=file)\n",
    "del file\n",
    "\n",
    "#The labels of the columns in the df data frame are having the prefix LambdaCandidates_ so we rename them\n",
    "new_labels= ['chi2geo', 'chi2primneg', 'chi2primpos', 'chi2topo', 'cosineneg',\n",
    "       'cosinepos', 'cosinetopo', 'distance', 'eta', 'l', 'ldl',\n",
    "       'mass', 'p', 'pT', 'phi', 'px', 'py', 'pz', 'rapidity',\n",
    "             'x', 'y', 'z', 'daughter1id', 'daughter2id', 'isfrompv', 'pid', 'issignal']\n",
    "\n",
    "sgnal.columns = new_labels\n",
    "bg.columns = new_labels\n",
    "df_original.columns=new_labels\n",
    "\n",
    "# Next we clean the data using the clean_df function saved in another .py file\n",
    "\n",
    "#Creating a new data frame and saving the results in it after cleaning of the original dfs\n",
    "#Also keeping the original one\n",
    "bcknd = clean_df(bg)\n",
    "signal = clean_df(sgnal)\n",
    "\n",
    "del bg\n",
    "del sgnal\n",
    "gc.collect()\n",
    "\n",
    "df_clean = clean_df(df_original)\n",
    "del df_original\n",
    "gc.collect()\n",
    "\n",
    "# We randomly choose our signal set of 4000 candidates\n",
    "signal_selected= signal.sample(n=90000)\n",
    "\n",
    "#background = 3 times the signal is also done randomly\n",
    "background_selected = bcknd\n",
    "\n",
    "del signal\n",
    "del bcknd\n",
    "\n",
    "#Let's combine signal and background\n",
    "dfs = [signal_selected, background_selected]\n",
    "df_scaled = pd.concat(dfs)\n",
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2primneg', 'chi2primpos', 'ldl', 'distance', 'chi2geo']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "x_whole = df_clean[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a new df \n",
    "new_check_set=KFPF_lambda_cuts(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>XGB Boost \n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,∞]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'eval_metric': 'auc', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=70, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })\n",
    "\n",
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=15, init_points=8, acq='ei')\n",
    "#0.9952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'], 'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), 'objective': 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst1= bst.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_test = pd.DataFrame(data=bst.predict(dtest1),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best, test_best = AMS(y_train, bst1,y_test, bst_test['xgb_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first argument should be a data frame, the second a column in it, in the form 'preds'\n",
    "preds_prob(bst_test,'xgb_preds', 'issignal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying XGB on the 10k events data-set\n",
    "df_clean['xgb_preds'] = bst.predict(dtest)\n",
    "preds_prob(df_clean,'xgb_preds', 'issignal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Already working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_visualization(cut, range1=(1.105, 1.19), bins1= 300 ):\n",
    "    mask1 = df_clean['xgb_preds']>cut\n",
    "    df3=df_clean[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(df_clean['mass'],bins = bins1, range=range1, facecolor='blue',alpha = 0.35, label='before selection')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc='upper left')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label='Machine learning (XGB)')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend( fontsize = 15,loc='upper right' )\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.title(\"The sample's Invariant Mass with XGB (with a cut > \"+str(cut)+')', fontsize = 15)\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_visualization(test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = test_best\n",
    "mask1 = df_clean['xgb_preds']>test_best\n",
    "df3=df_clean[mask1]\n",
    "fig, axs = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "#xgb\n",
    "\n",
    "df3_new = df3[df3['issignal']==1]\n",
    "df3_new1 = df3[df3['issignal']==0]\n",
    "df3['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True)\n",
    "#df3_new['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True)\n",
    "df3_new1['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True)\n",
    "plt.legend(('XGB selected lambdas','\\n False positives = \\n (MC =0)\\n background in \\n the distribution' ), fontsize = 18, loc='upper right')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"KFPF variables + cos$\\Theta_{between \\ \\overrightarrow{P_\\Lambda} \\  & \\ \\overrightarrow{P_{\\Pi^-}}}$ + $P_T$  with a cut of %.4f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "Thus in binary classification, the count of true positives is $C_{0,0}$, false positives is $C_{1,0}$, true negatives is $C_{1,1}$ and false negatives is $C_{0,1}$.\n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut1 = test_best\n",
    "df_clean['xgb_preds1'] = ((df_clean['xgb_preds']>cut1)*1)\n",
    "cnf_matrix = confusion_matrix(y_whole, df_clean['xgb_preds1'], labels=[1,0])\n",
    "#cnf_matrix = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut1))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Tree visualization\n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax1 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "xgb.plot_tree(bst,num_trees=10)\n",
    "plt.rcParams['figure.figsize'] = [20, 40]\n",
    "plt.rcParams['figure.dpi']=200\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.to_graphviz(xg_reg, fmap='', num_trees=0, rankdir=None, yes_color=None, no_color=None, condition_node_params=None, leaf_node_params=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = test_best\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3_base=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.0999, 1.17)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base['mass']),bins = 300, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 300, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost Selected $\\Lambda$s','KFPF selected $\\Lambda$s'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The lambda's Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3_base['mass'],range=(1.09, 1.17), bins=300)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(1.09, 1.17), bins=300)\n",
    "\n",
    "#makes sense to have only positive values \n",
    "diff = (hist1 - hist2)\n",
    "axs[1].bar(bins[:-1],     # this is what makes it comparable\n",
    "        ns / ns1, # maybe check for div-by-zero!\n",
    "        width=0.001)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define fit function.\n",
    "def fit_function(x, amplitude, mean, stddev,c,d,e):\n",
    "    return (amplitude) * (np.exp(-(0.5)*((x - mean) / stddev)**2))+c+d*x+e*(x**2)\n",
    "def background_function(x,c,d,e):\n",
    "    return c+d*x+e*(x**2)\n",
    "\n",
    "def signal_function( x, amplitude, mean, stddev):\n",
    "    return (amplitude) * (np.exp(-(0.5)*((x - mean) / stddev)**2))\n",
    "#def signal_function( x, x0, a, gam):\n",
    "#    return a * (gam**2) / ( gam**2 + ( x - x0 )**2)\n",
    "\n",
    "#def fit_function( x, x0, a, gam, c,d,e):\n",
    "#    return a * (gam**2) / ( gam**2 + ( x - x0 )**2)+(c+d*x+e*(x**2))\n",
    "\n",
    "# 3.) Generate exponential and gaussian data and histograms.\n",
    "data = df3['mass']\n",
    "bins = np.linspace(1.108, 1.126, 70)\n",
    "data_entries_1, bins_1 = np.histogram(data, bins=bins)\n",
    "\n",
    "# 4.) Add histograms of exponential and gaussian data.\n",
    "data_entries = data_entries_1 \n",
    "binscenters = np.array([0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)])\n",
    "\n",
    "# 5.) Fit the function to the histogram data.\n",
    "popt, pcov = curve_fit(fit_function, xdata=binscenters, ydata=data_entries, p0=[600,1.115,0.001,0,0,0])\n",
    "#popt, pcov = curve_fit(fit_function, xdata=binscenters, ydata=data_entries, p0=[1.1156,800,0.0013,107000,-100000,800000])\n",
    "print(popt)\n",
    "\n",
    "# 6.)\n",
    "# Generate enough x values to make the curves look smooth.\n",
    "xspace = np.linspace(1.108, 1.126, 1000000)\n",
    "fig, axs = plt.subplots(figsize=(15, 10))\n",
    "# Plot the histogram and the fitted function.\n",
    "plt.bar(binscenters, data_entries, width=bins[1] - bins[0], color='navy',alpha = 0.5, label=r'Histogram entries')\n",
    "plt.plot(xspace, fit_function(xspace, *popt), color='darkorange', linewidth=2.5, label=r'Fitted function')\n",
    "\n",
    "\n",
    "plt.plot(xspace,background_function(xspace,popt[3],popt[4],popt[5]),color='red')\n",
    "plt.plot(xspace,signal_function(xspace,popt[0],popt[1],popt[2]),color='green')\n",
    "\n",
    "# Make the plot nicer.\n",
    "#plt.xlim(1.108,1.124)\n",
    "plt.xlabel(r'Mass in $\\frac{GeV}{c^2}$', fontsize=15)\n",
    "plt.ylabel(r'Number of entries')\n",
    "plt.title(r'Lamda baryon')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "fig.savefig('hists.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "def integrand( x, x0, a, gam):\n",
    "    return a * (gam**2) / ( gam**2 + ( x - x0 )**2)\n",
    "\n",
    "x0 = popt[0]\n",
    "a = popt[1]\n",
    "gam = popt[2]\n",
    "I_sig = quad(integrand, popt[0]-5*popt[2], popt[0]+5*popt[2] , args=(x0, a, gam))\n",
    "I_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrand(x,c,d,e):\n",
    "    return c+d*x+e*(x**2)\n",
    "\n",
    "\n",
    "c = popt[3]\n",
    "d = popt[4]\n",
    "e = popt[5]\n",
    "I = quad(integrand, popt[0]-5*popt[2], popt[0]+5*popt[2] , args=(c,d,e))\n",
    "I_sig[0]/abs(I[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrand(x, amplitude, mean, stddev,c,d,e):\n",
    "    return (amplitude) * (np.exp(-(0.5)*((x - mean) / stddev)**2))+c+d*x+e*(x**2)\n",
    "\n",
    "amplitude = popt[0]\n",
    "mean = popt[1]\n",
    "stddev = popt[2]\n",
    "c = popt[3]\n",
    "d = popt[4]\n",
    "e = popt[5]\n",
    "I = quad(integrand, popt[1]-5*popt[2], popt[1]+5*popt[2] , args=(amplitude,mean,stddev,c,d,e))\n",
    "I_sig[0]/(np.sqrt(I[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define fit function.\n",
    "def fit_function(x, amp, mean, std,c,d,e):\n",
    "    return (amp/(np.sqrt(2*np.pi) * std)) * (np.exp(-(0.5)*((x - mean) / std)**2))+c+d*x+e*(x**2)\n",
    "def background_function(x,c,d,e):\n",
    "    return c+d*x+e*(x**2)\n",
    "\n",
    "def signal_function( x, amp, mean, std):\n",
    "    return (amp/(np.sqrt(2*np.pi) * std)) * (np.exp(-(0.5)*((x - mean) / std)**2))\n",
    "#def signal_function( x, a,mean, gam):\n",
    "#    return a * (gam**2) / ( gam**2 + ( x - mean )**2)\n",
    "\n",
    "#def fit_function( x, a, mean, gam, c,d,e):\n",
    "#    return a * (gam**2) / ( gam**2 + ( x - mean )**2)+(c+d*x)++e*(x**2)\n",
    "\n",
    "# 3.) Generate exponential and gaussian data and histograms.\n",
    "data = df3['mass']\n",
    "bins = np.linspace(1.105, 1.126, 70)\n",
    "data_entries_1, bins_1 = np.histogram(data, bins=bins)\n",
    "\n",
    "# 4.) Add histograms of exponential and gaussian data.\n",
    "data_entries = data_entries_1 \n",
    "binscenters = np.array([0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)])\n",
    "\n",
    "# 5.) Fit the function to the histogram data.\n",
    "popt, pcov = curve_fit(fit_function, xdata=binscenters, ydata=data_entries, p0=[600,1.115,0.001,0,0,0])\n",
    "#popt, pcov = curve_fit(fit_function, xdata=binscenters, ydata=data_entries, p0=[1,1.1156,0.001,0,0,0])\n",
    "print(popt)\n",
    "\n",
    "# 6.)\n",
    "# Generate enough x values to make the curves look smooth.\n",
    "xspace = np.linspace(1.105, 1.126, 1000000)\n",
    "fig, axs = plt.subplots(figsize=(15, 10))\n",
    "# Plot the histogram and the fitted function.\n",
    "plt.bar(binscenters, data_entries, width=bins[1] - bins[0], color='navy',alpha = 0.5, label=r'Histogram entries')\n",
    "plt.plot(xspace, fit_function(xspace, *popt), color='darkorange', linewidth=2.5, label=r'Fitted function')\n",
    "\n",
    "\n",
    "plt.plot(xspace,background_function(xspace,popt[3],popt[4],popt[5]),color='red')\n",
    "plt.plot(xspace,signal_function(xspace,popt[0],popt[1],popt[2]),color='green')\n",
    "\n",
    "# Make the plot nicer.\n",
    "#plt.xlim(1.108,1.124)\n",
    "plt.xlabel(r'Mass in $\\frac{GeV}{c^2}$', fontsize=15)\n",
    "plt.ylabel(r'Number of entries')\n",
    "plt.title(r'Lamda baryon')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "fig.savefig('hists.png')\n",
    "#pcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=binscenters, \n",
    "y=data_entries\n",
    "\n",
    "#def gaussian(x, amp, cen, wid, c,d,e):\n",
    "#    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "#    return ((amp / (np.sqrt(2*np.pi) * wid)) * np.exp(-(x-cen)**2 / (2*wid**2)))+c+d*x+e*(x**2)\n",
    "\n",
    "\n",
    "gmodel = Model(fit_function)\n",
    "result = gmodel.fit(y, x=x, amp=3, mean=1.115, std=0.0009, c=20, d=10, e=-10,  method='bfgs')\n",
    "#result = gmodel.fit(y, x=x, a=900, mean=1.1157, gam=0.0009, c=-100, d=100, e=10, method='cg')\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(15, 10))\n",
    "plt.bar(binscenters, data_entries, width=bins[1] - bins[0], color='navy',alpha = 0.5, label=r'Histogram entries')\n",
    "#plt.plot(x, result.init_fit, 'k--', label='initial fit')\n",
    "#plt.plot(x, result.best_fit, 'r-', label='best fit')\n",
    "plt.plot(xspace, fit_function(xspace, result.values['amp'],result.values['mean'],result.values['std']\n",
    "                          ,result.values['c'],result.values['d'],result.values['e']))\n",
    "\n",
    "plt.plot(xspace,background_function(xspace,result.values['c'],result.values['d'],result.values['e']),color='red')\n",
    "plt.plot(xspace,signal_function(xspace,result.values['amp'],result.values['mean'],result.values['std'])\n",
    "         ,color='green')\n",
    "#plt.legend(loc='best')\n",
    "#plt.show()\n",
    "# <end examp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
